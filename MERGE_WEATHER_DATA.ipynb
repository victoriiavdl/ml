{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE COMPLET : TRAIN + STATIONS MÉTÉO + DONNÉES MÉTÉO\n",
    "\n",
    "## DÉMARCHE:\n",
    "1. Charger train.csv (données par région et semaine avec TauxGrippe)\n",
    "2. Charger ListedesStationsMeteo.csv (liste des stations avec coordonnées)\n",
    "3. Charger tous les fichiers synop (données météo par station et date)\n",
    "4. Mapper chaque région à ses stations météo représentatives\n",
    "5. Agréger les données météo par région et par semaine\n",
    "6. Merger train.csv avec les données météo agrégées\n",
    "\n",
    "## PROBLÈMES À RÉSOUDRE:\n",
    "- train.csv : données par RÉGION et SEMAINE\n",
    "- synop : données par STATION et DATE (horaire/journalier)\n",
    "- Il faut :\n",
    "  - a) Convertir les dates des fichiers synop en semaines\n",
    "  - b) Mapper les stations aux régions\n",
    "  - c) Agréger les données météo (moyenne par région/semaine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MERGE COMPLET : TRAIN + STATIONS MÉTÉO + DONNÉES MÉTÉO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 1 : Charger les données de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/6] Chargement des données de base...\")\n",
    "\n",
    "# Train.csv\n",
    "df_train = pd.read_csv('data_origin/train.csv')\n",
    "print(f\"✓ train.csv : {df_train.shape}\")\n",
    "print(f\"  Colonnes : {df_train.columns.tolist()}\")\n",
    "print(f\"  Exemple :\\n{df_train.head(3)}\")\n",
    "\n",
    "# Liste des stations météo\n",
    "df_stations = pd.read_csv('data_origin/ListedesStationsMeteo.csv', sep=';')\n",
    "print(f\"\\n✓ ListedesStationsMeteo.csv : {df_stations.shape}\")\n",
    "print(f\"  Colonnes : {df_stations.columns.tolist()}\")\n",
    "print(f\"  Exemple :\\n{df_stations.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 2 : Créer le mapping Régions <-> Stations Météo\n",
    "\n",
    "C'est la partie CRITIQUE du merge!\n",
    "\n",
    "**Approche** : Chaque région française a des stations météo principales. On crée un mapping manuel basé sur la géographie.\n",
    "\n",
    "**Note** : Les codes régions dans train.csv correspondent aux anciennes régions françaises (avant 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/6] Mapping Régions -> Stations Météo...\")\n",
    "\n",
    "# Mapping manuel : Région -> Liste de stations météo principales\n",
    "REGION_STATION_MAPPING = {\n",
    "    'ALSACE': ['07190', '07280'],  # Strasbourg\n",
    "    'AQUITAINE': ['07510', '07630'],  # Bordeaux, Biarritz\n",
    "    'AUVERGNE': ['07460', '07380'],  # Clermont-Ferrand, Vichy\n",
    "    'BASSE-NORMANDIE': ['07027', '07139'],  # Caen\n",
    "    'BOURGOGNE': ['07280', '07255'],  # Dijon\n",
    "    'BRETAGNE': ['07110', '07117', '07130'],  # Brest, Rennes\n",
    "    'CENTRE': ['07255', '07149'],  # Orléans, Tours\n",
    "    'CHAMPAGNE-ARDENNE': ['07072', '07168'],  # Reims\n",
    "    'CORSE': ['07761', '07790'],  # Ajaccio, Bastia\n",
    "    'FRANCHE-COMTE': ['07299', '07280'],  # Besançon\n",
    "    'HAUTE-NORMANDIE': ['07037', '07020'],  # Rouen, Le Havre\n",
    "    'ILE-DE-FRANCE': ['07150', '07149'],  # Paris-Montsouris, Orly\n",
    "    'LANGUEDOC-ROUSSILLON': ['07630', '07643'],  # Montpellier, Perpignan\n",
    "    'LIMOUSIN': ['07434', '07335'],  # Limoges\n",
    "    'LORRAINE': ['07090', '07180'],  # Nancy, Metz\n",
    "    'MIDI-PYRENEES': ['07630', '07627'],  # Toulouse\n",
    "    'NORD-PAS-DE-CALAIS': ['07005', '07015'],  # Lille\n",
    "    'PAYS DE LA LOIRE': ['07222', '07130'],  # Nantes\n",
    "    'PICARDIE': ['07005', '07015'],  # Amiens\n",
    "    'POITOU-CHARENTES': ['07335', '07255'],  # Poitiers, La Rochelle\n",
    "    \"PROVENCE-ALPES-COTE D'AZUR\": ['07650', '07690'],  # Marseille, Nice\n",
    "    'RHONE-ALPES': ['07481', '07482'],  # Lyon\n",
    "}\n",
    "\n",
    "# Créer une table de mapping station -> région\n",
    "station_region_map = []\n",
    "for region, stations in REGION_STATION_MAPPING.items():\n",
    "    for station in stations:\n",
    "        station_region_map.append({\n",
    "            'numer_sta': station,\n",
    "            'region_name': region\n",
    "        })\n",
    "\n",
    "df_station_region = pd.DataFrame(station_region_map)\n",
    "print(f\"✓ Mapping créé : {len(df_station_region)} associations station-région\")\n",
    "print(f\"  Régions couvertes : {df_station_region['region_name'].nunique()}\")\n",
    "print(f\"  Exemple :\\n{df_station_region.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 3 : Charger les données météo (SYNOP)\n",
    "\n",
    "**Attention** : Les fichiers synop sont volumineux (~550MB au total)\n",
    "\n",
    "**Optimisation** : On filtre immédiatement par les stations d'intérêt pour réduire la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n[3/6] Chargement des données météo (synop)...\")\n\n# Lister tous les fichiers synop\nsynop_files = sorted(glob.glob('DonneesMeteorologiques/DonneesMeteorologiques/synop.*.csv'))\nprint(f\"✓ Fichiers synop trouvés : {len(synop_files)}\")\nif len(synop_files) > 0:\n    print(f\"  Premier : {synop_files[0].split('/')[-1]}\")\n    print(f\"  Dernier : {synop_files[-1].split('/')[-1]}\")\n\n# Préparer les stations d'intérêt\nstations_of_interest = df_station_region['numer_sta'].unique().tolist()\nprint(f\"  Stations d'intérêt : {len(stations_of_interest)}\")\n\n# CORRECTION: Essayer les deux formats (string et int)\nstations_int = [int(s) for s in stations_of_interest]\nprint(f\"  Format string: {stations_of_interest[:3]}\")\nprint(f\"  Format int: {stations_int[:3]}\")\n\nsynop_data_list = []\nfor i, file in enumerate(synop_files):\n    if i % 12 == 0:  # Afficher progression tous les 12 mois\n        print(f\"  Chargement : {file.split('/')[-1]}...\")\n    \n    try:\n        df_synop = pd.read_csv(file, sep=';', low_memory=False)\n        \n        # CORRECTION: Essayer d'abord avec le format tel quel\n        df_synop_filtered = df_synop[df_synop['numer_sta'].isin(stations_of_interest)]\n        \n        # Si rien trouvé, essayer avec int\n        if len(df_synop_filtered) == 0:\n            df_synop_filtered = df_synop[df_synop['numer_sta'].isin(stations_int)]\n        \n        if len(df_synop_filtered) > 0:\n            synop_data_list.append(df_synop_filtered)\n            \n    except Exception as e:\n        print(f\"  ⚠ Erreur lecture {file} : {e}\")\n\n# Vérifier qu'on a des données\nif len(synop_data_list) == 0:\n    print(\"\\n❌ ERREUR: Aucune donnée chargée!\")\n    print(\"   Causes possibles:\")\n    print(\"   1. Les IDs de station ne correspondent pas\")\n    print(\"   2. Les fichiers synop sont vides ou corrompus\")\n    print(\"   3. Les stations du mapping n'existent pas dans les fichiers synop\")\n    print(\"\\n   → Vérifiez la cellule de débogage précédente!\")\n    raise ValueError(\"Aucune donnée synop chargée. Vérifiez le format des IDs de station.\")\n\n# Concaténer tous les fichiers\ndf_synop_all = pd.concat(synop_data_list, ignore_index=True)\nprint(f\"\\n✓ Données synop chargées : {df_synop_all.shape}\")\nprint(f\"  Colonnes : {len(df_synop_all.columns)}\")\nprint(f\"  Période : {df_synop_all['date'].min()} -> {df_synop_all['date'].max()}\")\nprint(f\"  Stations uniques : {df_synop_all['numer_sta'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ÉTAPE 2.5 : DÉBOGAGE - Vérifier le format des IDs de station\n\n**IMPORTANT** : Les IDs peuvent être en format différent (string vs int, avec/sans padding)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/6] Chargement des données météo (synop)...\")\n",
    "\n",
    "# Lister tous les fichiers synop\n",
    "synop_files = sorted(glob.glob('DonneesMeteorologiques/DonneesMeteorologiques/synop.*.csv'))\n",
    "print(f\"✓ Fichiers synop trouvés : {len(synop_files)}\")\n",
    "print(f\"  Premier : {synop_files[0].split('/')[-1]}\")\n",
    "print(f\"  Dernier : {synop_files[-1].split('/')[-1]}\")\n",
    "\n",
    "# Filtrer par les stations d'intérêt\n",
    "stations_of_interest = df_station_region['numer_sta'].unique().tolist()\n",
    "print(f\"  Stations d'intérêt : {len(stations_of_interest)}\")\n",
    "\n",
    "synop_data_list = []\n",
    "for i, file in enumerate(synop_files):\n",
    "    if i % 12 == 0:  # Afficher progression tous les 12 mois\n",
    "        print(f\"  Chargement : {file.split('/')[-1]}...\")\n",
    "    \n",
    "    try:\n",
    "        df_synop = pd.read_csv(file, sep=';', low_memory=False)\n",
    "        # Filtrer uniquement les stations d'intérêt\n",
    "        df_synop = df_synop[df_synop['numer_sta'].isin(stations_of_interest)]\n",
    "        if len(df_synop) > 0:\n",
    "            synop_data_list.append(df_synop)\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Erreur lecture {file} : {e}\")\n",
    "\n",
    "# Concaténer tous les fichiers\n",
    "df_synop_all = pd.concat(synop_data_list, ignore_index=True)\n",
    "print(f\"\\n✓ Données synop chargées : {df_synop_all.shape}\")\n",
    "print(f\"  Colonnes : {len(df_synop_all.columns)}\")\n",
    "print(f\"  Période : {df_synop_all['date'].min()} -> {df_synop_all['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 4 : Convertir les dates en semaines\n",
    "\n",
    "**Problème** : synop a des dates au format AAAAMMJJHHMMSS, train.csv a des semaines au format AAAASS\n",
    "\n",
    "**Solution** : Utiliser ISO calendar pour extraire l'année et le numéro de semaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[5/6] Agrégation des données météo par région et semaine...\")\n\n# CORRECTION: S'assurer que les IDs sont au même format pour le merge\n# Convertir numer_sta en string dans les deux DataFrames\ndf_synop_all['numer_sta'] = df_synop_all['numer_sta'].astype(str).str.zfill(5)\ndf_station_region['numer_sta'] = df_station_region['numer_sta'].astype(str).str.zfill(5)\n\n# Merger avec le mapping station->région\ndf_synop_all = df_synop_all.merge(df_station_region, on='numer_sta', how='inner')\nprint(f\"✓ Merge avec mapping : {df_synop_all.shape}\")\n\nif len(df_synop_all) == 0:\n    print(\"\\n❌ ERREUR: Le merge a échoué! Aucune correspondance trouvée.\")\n    print(\"   → Vérifiez le format des IDs dans la cellule de débogage\")\n    raise ValueError(\"Merge échoué entre synop et mapping stations\")\n\n# Sélectionner les variables météo importantes\nmeteo_vars = [\n    'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas',\n    'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24',\n    'tminsol', 'tw', 'raf10', 'rafper', 'per', 'ht_neige', 'ssfrai',\n    'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24'\n]\n\n# Vérifier quelles variables existent\nmeteo_vars_available = [v for v in meteo_vars if v in df_synop_all.columns]\nprint(f\"  Variables météo disponibles : {len(meteo_vars_available)}/{len(meteo_vars)}\")\n\n# Convertir les variables météo en numérique (remplacer 'mq' par NaN)\nfor var in meteo_vars_available:\n    df_synop_all[var] = pd.to_numeric(df_synop_all[var], errors='coerce')\n\n# Agréger par région et semaine (moyenne)\nagg_dict = {var: 'mean' for var in meteo_vars_available}\ndf_meteo_agg = df_synop_all.groupby(['region_name', 'week_year'], as_index=False).agg(agg_dict)\n\nprint(f\"✓ Agrégation effectuée : {df_meteo_agg.shape}\")\nprint(f\"  Régions uniques : {df_meteo_agg['region_name'].nunique()}\")\nprint(f\"  Semaines uniques : {df_meteo_agg['week_year'].nunique()}\")\nprint(f\"\\n  Exemple :\\n{df_meteo_agg.head(3)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 5 : Agréger les données météo par région et semaine\n",
    "\n",
    "**Approche** :\n",
    "1. Merger synop avec le mapping station->région\n",
    "2. Grouper par (région, semaine)\n",
    "3. Calculer la moyenne de chaque variable météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/6] Agrégation des données météo par région et semaine...\")\n",
    "\n",
    "# Merger avec le mapping station->région\n",
    "df_synop_all = df_synop_all.merge(df_station_region, on='numer_sta', how='inner')\n",
    "print(f\"✓ Merge avec mapping : {df_synop_all.shape}\")\n",
    "\n",
    "# Sélectionner les variables météo importantes\n",
    "meteo_vars = [\n",
    "    'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas',\n",
    "    'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24',\n",
    "    'tminsol', 'tw', 'raf10', 'rafper', 'per', 'ht_neige', 'ssfrai',\n",
    "    'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24'\n",
    "]\n",
    "\n",
    "# Vérifier quelles variables existent\n",
    "meteo_vars_available = [v for v in meteo_vars if v in df_synop_all.columns]\n",
    "print(f\"  Variables météo disponibles : {len(meteo_vars_available)}/{len(meteo_vars)}\")\n",
    "\n",
    "# Convertir les variables météo en numérique (remplacer 'mq' par NaN)\n",
    "for var in meteo_vars_available:\n",
    "    df_synop_all[var] = pd.to_numeric(df_synop_all[var], errors='coerce')\n",
    "\n",
    "# Agréger par région et semaine (moyenne)\n",
    "agg_dict = {var: 'mean' for var in meteo_vars_available}\n",
    "df_meteo_agg = df_synop_all.groupby(['region_name', 'week_year'], as_index=False).agg(agg_dict)\n",
    "\n",
    "print(f\"✓ Agrégation effectuée : {df_meteo_agg.shape}\")\n",
    "print(f\"  Exemple :\\n{df_meteo_agg.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÉTAPE 6 : Merge final avec train.csv\n",
    "\n",
    "**Clés de jointure** :\n",
    "- region_name (normalisé en majuscules)\n",
    "- week (de train.csv) = week_year (de synop agrégé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/6] Merge final avec train.csv...\")\n",
    "\n",
    "# Normaliser les noms de régions pour le merge\n",
    "df_train['region_name_clean'] = df_train['region_name'].str.upper().str.strip()\n",
    "df_meteo_agg['region_name_clean'] = df_meteo_agg['region_name'].str.upper().str.strip()\n",
    "\n",
    "# Merger\n",
    "df_final = df_train.merge(\n",
    "    df_meteo_agg,\n",
    "    left_on=['region_name_clean', 'week'],\n",
    "    right_on=['region_name_clean', 'week_year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Merge effectué : {df_final.shape}\")\n",
    "print(f\"  Colonnes : {len(df_final.columns)}\")\n",
    "\n",
    "# Nettoyer les colonnes dupliquées\n",
    "cols_to_drop = ['region_name_clean', 'week_year', 'region_name_y']\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "# Renommer region_name_x en region_name\n",
    "if 'region_name_x' in df_final.columns:\n",
    "    df_final = df_final.rename(columns={'region_name_x': 'region_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du fichier final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data_plus/train_weather_merged_complete.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MERGE TERMINÉ AVEC SUCCÈS!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFichier généré : {output_file}\")\n",
    "print(f\"  Dimensions : {df_final.shape}\")\n",
    "print(f\"  Observations : {len(df_final)}\")\n",
    "print(f\"  Variables météo ajoutées : {len(meteo_vars_available)}\")\n",
    "print(f\"\\nAperçu final :\")\n",
    "print(df_final.head(3))\n",
    "\n",
    "print(f\"\\nColonnes finales ({len(df_final.columns)}) :\")\n",
    "print(df_final.columns.tolist())\n",
    "\n",
    "print(f\"\\n✓ Le fichier est prêt pour l'analyse et la modélisation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse du résultat\n",
    "\n",
    "Vérifions la couverture et la qualité du merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyse du merge :\")\n",
    "print(f\"  train.csv original : {len(df_train)} lignes\")\n",
    "print(f\"  Après merge : {len(df_final)} lignes\")\n",
    "print(f\"  Taux de couverture : {len(df_final)/len(df_train)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nRégions couvertes :\")\n",
    "print(df_final['region_name'].value_counts())\n",
    "\n",
    "print(f\"\\nPériode couverte :\")\n",
    "print(f\"  Semaines : {df_final['week'].min()} -> {df_final['week'].max()}\")\n",
    "\n",
    "print(f\"\\nValeurs manquantes :\")\n",
    "missing = df_final.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "print(missing.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}