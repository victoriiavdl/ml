{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ G√âN√âRATION DES PR√âDICTIONS KAGGLE\n",
    "\n",
    "**Ce notebook**:\n",
    "1. Charge les mod√®les entra√Æn√©s\n",
    "2. Pr√©pare le test set (2012-2013) avec les m√™mes features\n",
    "3. G√©n√®re les pr√©dictions\n",
    "4. Cr√©e le fichier submission.csv\n",
    "\n",
    "**‚ö†Ô∏è Pr√©requis**: Avoir ex√©cut√© KAGGLE_TRAIN_MODEL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports r√©ussis!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CHARGEMENT DES MOD√àLES ENTRA√éN√âS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement des mod√®les...\n",
      "‚úì Mod√®les charg√©s!\n",
      "‚úì Features: 26\n",
      "‚úì Poids ensemble: {'XGBoost': np.float64(0.32493732368191636), 'LightGBM': np.float64(0.3272881517152026), 'CatBoost': np.float64(0.3477745246028811)}\n"
     ]
    }
   ],
   "source": [
    "# Charger les mod√®les\n",
    "print(\"üì¶ Chargement des mod√®les...\")\n",
    "with open('xgb_final.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "with open('lgb_final.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "with open('cat_final.pkl', 'rb') as f:\n",
    "    cat_model = pickle.load(f)\n",
    "with open('imputer.pkl', 'rb') as f:\n",
    "    imputer = pickle.load(f)\n",
    "with open('weights.pkl', 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Mod√®les charg√©s!\")\n",
    "print(f\"‚úì Features: {len(features)}\")\n",
    "print(f\"‚úì Poids ensemble: {weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CHARGEMENT DU TEST SET\n",
    "\n",
    "**Important**: On doit d'abord merger le test avec les donn√©es m√©t√©o 2012-2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Id      week  region_code      region_name  week_year      tend  \\\n",
      "0  3235  201352.0           42           ALSACE     201352  3.571429   \n",
      "1  3236  201352.0           72        AQUITAINE     201352 -1.694139   \n",
      "2  3237  201352.0           83         AUVERGNE     201352  3.035714   \n",
      "3  3238  201352.0           25  BASSE-NORMANDIE     201352  8.035714   \n",
      "4  3239  201352.0           26        BOURGOGNE     201352  1.261905   \n",
      "\n",
      "           dd        ff           t          td  ...     rafper   per  \\\n",
      "0  212.321429  4.460714  282.805357  279.551786  ...   8.400000 -10.0   \n",
      "1  172.884615  4.378022  280.476282  277.717949  ...   8.811401 -10.0   \n",
      "2  211.250000  4.996429  279.396429  277.405357  ...   8.851786 -10.0   \n",
      "3  216.005411  7.758885  281.120833  278.380952  ...  13.909942 -10.0   \n",
      "4  192.097403  5.424859  280.119762  275.674167  ...  10.343972 -10.0   \n",
      "\n",
      "   ht_neige  ssfrai  perssfrai       rr1       rr3       rr6      rr12  \\\n",
      "0       0.0     0.0 -46.595745  0.346429  0.942857  1.842857  4.014286   \n",
      "1       0.0     0.0 -43.200000  0.189659  0.484235  1.149512  2.466911   \n",
      "2       0.0     0.0 -44.347826  0.278571  0.739286  1.242857  3.000000   \n",
      "3       0.0     0.0 -45.326087  0.328420  0.789622  1.810623  2.816117   \n",
      "4       0.0     0.0 -45.419287  0.228182  0.593658  1.490476  2.342857   \n",
      "\n",
      "       rr24  \n",
      "0  7.042857  \n",
      "1  5.607937  \n",
      "2  6.085714  \n",
      "3  5.593651  \n",
      "4  4.595238  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "Index(['Id', 'week', 'region_code', 'region_name', 'week_year', 'tend', 'dd',\n",
      "       'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas', 'pres', 'tn12', 'tx12',\n",
      "       'tminsol', 'rafper', 'per', 'ht_neige', 'ssfrai', 'perssfrai', 'rr1',\n",
      "       'rr3', 'rr6', 'rr12', 'rr24'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2288 entries, 0 to 2287\n",
      "Data columns (total 29 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Id           2288 non-null   int64  \n",
      " 1   week         2288 non-null   float64\n",
      " 2   region_code  2288 non-null   int64  \n",
      " 3   region_name  2288 non-null   object \n",
      " 4   week_year    2288 non-null   int64  \n",
      " 5   tend         2288 non-null   float64\n",
      " 6   dd           2288 non-null   float64\n",
      " 7   ff           2288 non-null   float64\n",
      " 8   t            2288 non-null   float64\n",
      " 9   td           2288 non-null   float64\n",
      " 10  u            2288 non-null   float64\n",
      " 11  vv           2288 non-null   float64\n",
      " 12  n            2288 non-null   float64\n",
      " 13  nbas         2288 non-null   float64\n",
      " 14  hbas         2288 non-null   float64\n",
      " 15  pres         2288 non-null   float64\n",
      " 16  tn12         2288 non-null   float64\n",
      " 17  tx12         2288 non-null   float64\n",
      " 18  tminsol      2288 non-null   float64\n",
      " 19  rafper       2288 non-null   float64\n",
      " 20  per          2288 non-null   float64\n",
      " 21  ht_neige     2288 non-null   float64\n",
      " 22  ssfrai       2288 non-null   float64\n",
      " 23  perssfrai    2288 non-null   float64\n",
      " 24  rr1          2288 non-null   float64\n",
      " 25  rr3          2288 non-null   float64\n",
      " 26  rr6          2288 non-null   float64\n",
      " 27  rr12         2288 non-null   float64\n",
      " 28  rr24         2288 non-null   float64\n",
      "dtypes: float64(25), int64(3), object(1)\n",
      "memory usage: 518.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# OPTION 1: Si vous avez d√©j√† un fichier test_synop_merged.csv\n",
    "df_test = pd.read_csv('data_plus/test_synop_merged.csv')\n",
    "\n",
    "print (df_test.head())\n",
    "print (df_test.columns)\n",
    "print (df_test.info())\n",
    "\n",
    "# OPTION 2: Charger le test brut et le merger avec les donn√©es m√©t√©o\n",
    "# (Code de merge √† ajouter - similaire au train)\n",
    "\n",
    "# Pour l'instant, on suppose que vous devez cr√©er ce fichier\n",
    "# print(\"‚ö†Ô∏è IMPORTANT:\")\n",
    "# print(\"Vous devez d'abord cr√©er le test set avec les donn√©es m√©t√©o 2012-2013\")\n",
    "# print(\"Voir le script de pr√©paration du test ci-dessous...\")\n",
    "\n",
    "# PLACEHOLDER - √Ä adapter selon votre structure\n",
    "# df_test = pd.read_csv('data_origin/test.csv')\n",
    "# Puis merger avec donn√©es m√©t√©o 2012-2013 comme fait pour le train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PR√âPARATION DU TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonction de pr√©paration du test d√©finie\n"
     ]
    }
   ],
   "source": [
    "# Cette cellule suppose que df_test est charg√© et a les colonnes m√©t√©o\n",
    "# Similaires √† df_train_full\n",
    "\n",
    "def prepare_test_set(df_test, df_train_full):\n",
    "    \"\"\"\n",
    "    Pr√©parer le test set avec les M√äMES features que le train\n",
    "    \"\"\"\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    # Convertir date\n",
    "    # Cr√©er date √† partir de week_year si n√©cessaire\n",
    "    def week_to_datetime(week_str):\n",
    "        year = int(str(week_str)[:4])\n",
    "        week = int(str(week_str)[4:6])\n",
    "        from datetime import datetime\n",
    "        jan4 = datetime(year, 1, 4)\n",
    "        week_one_monday = jan4 - pd.Timedelta(days=jan4.weekday())\n",
    "        return week_one_monday + pd.Timedelta(weeks=week-1)\n",
    "    \n",
    "    if 'date' not in df_test.columns:\n",
    "        df_test['date'] = df_test['week'].apply(week_to_datetime)\n",
    "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "    \n",
    "    # Ajouter saison\n",
    "    def get_season(date):\n",
    "        month = date.month\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Hiver'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Printemps'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Ete'\n",
    "        else:\n",
    "            return 'Automne'\n",
    "    \n",
    "    df_test['saison'] = df_test['date'].apply(get_season)\n",
    "    \n",
    "    # Features temporelles\n",
    "    df_test['year'] = df_test['date'].dt.year\n",
    "    df_test['month'] = df_test['date'].dt.month\n",
    "    df_test['week_of_year'] = df_test['date'].dt.isocalendar().week\n",
    "    df_test['day_of_year'] = df_test['date'].dt.dayofyear\n",
    "    \n",
    "    # Features cycliques\n",
    "    df_test['week_sin'] = np.sin(2 * np.pi * df_test['week_of_year'] / 52)\n",
    "    df_test['week_cos'] = np.cos(2 * np.pi * df_test['week_of_year'] / 52)\n",
    "    df_test['month_sin'] = np.sin(2 * np.pi * df_test['month'] / 12)\n",
    "    df_test['month_cos'] = np.cos(2 * np.pi * df_test['month'] / 12)\n",
    "    \n",
    "    # Encoder saison\n",
    "    saison_map = {'Hiver': 1, 'Printemps': 2, 'Ete': 3, 'Automne': 4}\n",
    "    df_test['saison_encoded'] = df_test['saison'].map(saison_map)\n",
    "    \n",
    "    # üéØ FEATURES HISTORIQUES - Calcul√©es depuis le TRAIN\n",
    "    # Cr√©er les mappings depuis le train\n",
    "    hist_week_map = df_train_full.groupby(['region_code', 'week_of_year'])['TauxGrippe'].mean().to_dict()\n",
    "    hist_month_map = df_train_full.groupby(['region_code', 'month'])['TauxGrippe'].mean().to_dict()\n",
    "    hist_season_map = df_train_full.groupby(['region_code', 'saison'])['TauxGrippe'].mean().to_dict()\n",
    "    region_mean_map = df_train_full.groupby('region_code')['TauxGrippe'].mean().to_dict()\n",
    "    region_std_map = df_train_full.groupby('region_code')['TauxGrippe'].std().to_dict()\n",
    "    week_global_map = df_train_full.groupby('week_of_year')['TauxGrippe'].mean().to_dict()\n",
    "    \n",
    "    # Appliquer au test\n",
    "    df_test['TauxGrippe_hist_week_mean'] = df_test.apply(\n",
    "        lambda x: hist_week_map.get((x['region_code'], x['week_of_year']), region_mean_map.get(x['region_code'], 0)),\n",
    "        axis=1\n",
    "    )\n",
    "    df_test['TauxGrippe_hist_month_mean'] = df_test.apply(\n",
    "        lambda x: hist_month_map.get((x['region_code'], x['month']), region_mean_map.get(x['region_code'], 0)),\n",
    "        axis=1\n",
    "    )\n",
    "    df_test['TauxGrippe_hist_season_mean'] = df_test.apply(\n",
    "        lambda x: hist_season_map.get((x['region_code'], x['saison']), region_mean_map.get(x['region_code'], 0)),\n",
    "        axis=1\n",
    "    )\n",
    "    df_test['TauxGrippe_region_mean'] = df_test['region_code'].map(region_mean_map)\n",
    "    df_test['TauxGrippe_region_std'] = df_test['region_code'].map(region_std_map)\n",
    "    df_test['TauxGrippe_week_global_mean'] = df_test['week_of_year'].map(week_global_map)\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "print(\"‚úì Fonction de pr√©paration du test d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger le train pour les stats historiques\ndf_train_full = pd.read_csv('data_plus/train_synop_cleaned_complet.csv')\ndf_train_full['date'] = pd.to_datetime(df_train_full['date'])\n\n# IMPORTANT: Cr√©er les features temporelles sur le train AVANT de l'utiliser\nprint(\"üìä Cr√©ation des features temporelles sur le train...\")\n\n# Features temporelles\ndf_train_full['year'] = df_train_full['date'].dt.year\ndf_train_full['month'] = df_train_full['date'].dt.month\ndf_train_full['week_of_year'] = df_train_full['date'].dt.isocalendar().week\n\nprint(f\"‚úì Train pr√©par√©: {df_train_full.shape}\")\n\n# Charger le test set\ndf_test = pd.read_csv('data_plus/test_synop_merged.csv')\n\n# Pr√©parer le test avec les m√™mes transformations\ndf_test = prepare_test_set(df_test, df_train_full)\n\nprint(f\"‚úì Test set pr√©par√©: {df_test.shape}\")\nprint(f\"Colonnes test: {df_test.columns.tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. G√âN√âRATION DES PR√âDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['week_of_year', 'month', 'week_sin', 'week_cos', 'month_sin', 'month_cos', 'saison_encoded', 'TauxGrippe_hist_week_mean', 'TauxGrippe_hist_month_mean', 'TauxGrippe_hist_season_mean', 'TauxGrippe_region_mean', 'TauxGrippe_region_std', 'TauxGrippe_week_global_mean'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cette cellule suppose que df_test est pr√©par√©\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Pr√©parer X_test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_test = \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m test_ids = df_test[\u001b[33m'\u001b[39m\u001b[33mId\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Imputer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['week_of_year', 'month', 'week_sin', 'week_cos', 'month_sin', 'month_cos', 'saison_encoded', 'TauxGrippe_hist_week_mean', 'TauxGrippe_hist_month_mean', 'TauxGrippe_hist_season_mean', 'TauxGrippe_region_mean', 'TauxGrippe_region_std', 'TauxGrippe_week_global_mean'] not in index\""
     ]
    }
   ],
   "source": [
    "# Cette cellule suppose que df_test est pr√©par√©\n",
    "\n",
    "# Pr√©parer X_test\n",
    "X_test = df_test[features]\n",
    "test_ids = df_test['Id']\n",
    "\n",
    "# Imputer\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"‚úì Test set pr√©par√©: {X_test.shape}\")\n",
    "\n",
    "# Pr√©dictions avec les 3 mod√®les\n",
    "print(\"\\nüöÄ G√©n√©ration des pr√©dictions...\")\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"‚úì XGBoost\")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "print(\"‚úì LightGBM\")\n",
    "\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "print(\"‚úì CatBoost\")\n",
    "\n",
    "# Ensemble (moyenne pond√©r√©e)\n",
    "y_pred = (\n",
    "    weights['XGBoost'] * y_pred_xgb +\n",
    "    weights['LightGBM'] * y_pred_lgb +\n",
    "    weights['CatBoost'] * y_pred_cat\n",
    ")\n",
    "print(\"‚úì Ensemble\")\n",
    "\n",
    "# S'assurer que les pr√©dictions sont positives\n",
    "y_pred = np.maximum(y_pred, 0)\n",
    "\n",
    "print(f\"\\nüìä Statistiques des pr√©dictions:\")\n",
    "print(f\"   Min: {y_pred.min():.2f}\")\n",
    "print(f\"   Max: {y_pred.max():.2f}\")\n",
    "print(f\"   Moyenne: {y_pred.mean():.2f}\")\n",
    "print(f\"   M√©diane: {np.median(y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CR√âATION DU FICHIER SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le fichier submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'TauxGrippe': y_pred\n",
    "})\n",
    "\n",
    "# Sauvegarder\n",
    "submission.to_csv('submission_ensemble.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FICHIER SUBMISSION CR√â√â!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Fichier: submission_ensemble.csv\")\n",
    "print(f\"Lignes: {len(submission)}\")\n",
    "print(f\"\\nAper√ßu:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nüéØ Pr√™t √† soumettre sur Kaggle!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (OPTIONNEL) CR√âER DES SUBMISSIONS INDIVIDUELLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous voulez tester les mod√®les individuellement\n",
    "\n",
    "# XGBoost\n",
    "submission_xgb = pd.DataFrame({'Id': test_ids, 'TauxGrippe': np.maximum(y_pred_xgb, 0)})\n",
    "submission_xgb.to_csv('submission_xgb.csv', index=False)\n",
    "\n",
    "# LightGBM\n",
    "submission_lgb = pd.DataFrame({'Id': test_ids, 'TauxGrippe': np.maximum(y_pred_lgb, 0)})\n",
    "submission_lgb.to_csv('submission_lgb.csv', index=False)\n",
    "\n",
    "# CatBoost\n",
    "submission_cat = pd.DataFrame({'Id': test_ids, 'TauxGrippe': np.maximum(y_pred_cat, 0)})\n",
    "submission_cat.to_csv('submission_cat.csv', index=False)\n",
    "\n",
    "print(\"‚úì Submissions individuelles cr√©√©es:\")\n",
    "print(\"  - submission_xgb.csv\")\n",
    "print(\"  - submission_lgb.csv\")\n",
    "print(\"  - submission_cat.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}