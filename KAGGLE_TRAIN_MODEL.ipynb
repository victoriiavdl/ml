{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ KAGGLE INFLUENZA PREDICTION - STRAT√âGIE OPTIMIS√âE\n",
    "\n",
    "**Objectif**: Maximiser le score Kaggle\n",
    "\n",
    "**Strat√©gie**:\n",
    "- Features m√©t√©o + Features temporelles cycliques + **Moyennes historiques**\n",
    "- Validation sur 2011 (mimique le test 2012-2013)\n",
    "- Ensemble XGBoost + LightGBM + CatBoost\n",
    "\n",
    "**Structure**:\n",
    "- Train: 2004-2010\n",
    "- Validation: 2011\n",
    "- Test: 2012-2013 (√† pr√©dire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Imports r√©ussis!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CHARGEMENT DES DONN√âES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Charger le dataset nettoy√©\n",
    "df_train_full = pd.read_csv('data_plus/train_synop_cleaned_complet.csv')\n",
    "df_train_full['date'] = pd.to_datetime(df_train_full['date'])\n",
    "\n",
    "print(f\"‚úì Dataset charg√©: {df_train_full.shape}\")\n",
    "print(f\"P√©riode: {df_train_full['date'].min()} √† {df_train_full['date'].max()}\")\n",
    "print(f\"R√©gions: {df_train_full['region_code'].nunique()}\")\n",
    "df_train_full.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FEATURE ENGINEERING STRAT√âGIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"Features temporelles avec cyclicit√©\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Composantes temporelles\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Features cycliques (IMPORTANT pour saisonnalit√©)\n",
    "    df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Encoder saison\n",
    "    saison_map = {'Hiver': 1, 'Printemps': 2, 'Ete': 3, 'Automne': 4}\n",
    "    df['saison_encoded'] = df['saison'].map(saison_map)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_historical_features(df):\n",
    "    \"\"\"\n",
    "    üöÄ SECRET WEAPON: Moyennes historiques\n",
    "    Capture les patterns saisonniers de chaque r√©gion\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Moyenne par r√©gion + semaine de l'ann√©e\n",
    "    df['TauxGrippe_hist_week_mean'] = df.groupby(['region_code', 'week_of_year'])['TauxGrippe'].transform('mean')\n",
    "    \n",
    "    # 2. Moyenne par r√©gion + mois\n",
    "    df['TauxGrippe_hist_month_mean'] = df.groupby(['region_code', 'month'])['TauxGrippe'].transform('mean')\n",
    "    \n",
    "    # 3. Moyenne par r√©gion + saison\n",
    "    df['TauxGrippe_hist_season_mean'] = df.groupby(['region_code', 'saison'])['TauxGrippe'].transform('mean')\n",
    "    \n",
    "    # 4. Stats globales par r√©gion\n",
    "    df['TauxGrippe_region_mean'] = df.groupby('region_code')['TauxGrippe'].transform('mean')\n",
    "    df['TauxGrippe_region_std'] = df.groupby('region_code')['TauxGrippe'].transform('std')\n",
    "    \n",
    "    # 5. Stats globales par semaine\n",
    "    df['TauxGrippe_week_global_mean'] = df.groupby('week_of_year')['TauxGrippe'].transform('mean')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer\n",
    "print(\"üìä Cr√©ation des features temporelles...\")\n",
    "df_train_full = create_temporal_features(df_train_full)\n",
    "\n",
    "print(\"üéØ Cr√©ation des features historiques...\")\n",
    "df_train_full = create_historical_features(df_train_full)\n",
    "\n",
    "print(f\"‚úì Features cr√©√©es. Shape: {df_train_full.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S√âLECTION DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Features m√©t√©o\n",
    "meteo_features = ['t', 'u', 'td', 'ff', 'vv', 'tminsol', 'pres', \n",
    "                  'rr3', 'rr6', 'rr12', 'rr24', 'n']\n",
    "\n",
    "# Features temporelles\n",
    "temporal_features = ['week_of_year', 'month', 'week_sin', 'week_cos', \n",
    "                     'month_sin', 'month_cos', 'saison_encoded']\n",
    "\n",
    "# Features historiques\n",
    "historical_features = ['TauxGrippe_hist_week_mean', 'TauxGrippe_hist_month_mean',\n",
    "                       'TauxGrippe_hist_season_mean', 'TauxGrippe_region_mean',\n",
    "                       'TauxGrippe_region_std', 'TauxGrippe_week_global_mean']\n",
    "\n",
    "# R√©gion\n",
    "region_features = ['region_code']\n",
    "\n",
    "# Toutes les features\n",
    "all_features = meteo_features + temporal_features + historical_features + region_features\n",
    "available_features = [f for f in all_features if f in df_train_full.columns]\n",
    "\n",
    "print(f\"‚úì {len(available_features)} features s√©lectionn√©es\")\n",
    "print(f\"  M√©t√©o: {len([f for f in meteo_features if f in available_features])}\")\n",
    "print(f\"  Temporelles: {len([f for f in temporal_features if f in available_features])}\")\n",
    "print(f\"  Historiques: {len([f for f in historical_features if f in available_features])}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SPLIT DE VALIDATION STRAT√âGIQUE\n",
    "\n",
    "**Strat√©gie**: Utiliser 2011 comme validation pour mimique le test (2012-2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split: Train 2004-2010, Validation 2011\n",
    "df_train = df_train_full[df_train_full['year'] <= 2010].copy()\n",
    "df_val = df_train_full[df_train_full['year'] == 2011].copy()\n",
    "\n",
    "print(f\"‚úì Train: {df_train.shape[0]} obs ({df_train['year'].min()}-{df_train['year'].max()})\")\n",
    "print(f\"‚úì Validation: {df_val.shape[0]} obs ({df_val['year'].min()})\")\n",
    "\n",
    "# Pr√©parer X, y\n",
    "X_train = df_train[available_features]\n",
    "y_train = df_train['TauxGrippe']\n",
    "X_val = df_val[available_features]\n",
    "y_val = df_val['TauxGrippe']\n",
    "\n",
    "# Imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "\n",
    "print(f\"‚úì Donn√©es pr√©par√©es\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ENTRA√éNEMENT DES MOD√àLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "results = {}\n",
    "\n",
    "# XGBoost\n",
    "print(\"üöÄ [1/3] XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500, max_depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, min_child_weight=3,\n",
    "    gamma=0.1, random_state=42, n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "              early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_val_pred_xgb))\n",
    "results['XGBoost'] = {'RMSE': rmse_xgb, 'R¬≤': r2_score(y_val, y_val_pred_xgb)}\n",
    "print(f\"‚úì XGBoost - RMSE: {rmse_xgb:.2f}\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"‚ö° [2/3] LightGBM...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=500, max_depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, min_child_samples=20,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "              callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "\n",
    "y_val_pred_lgb = lgb_model.predict(X_val)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, y_val_pred_lgb))\n",
    "results['LightGBM'] = {'RMSE': rmse_lgb, 'R¬≤': r2_score(y_val, y_val_pred_lgb)}\n",
    "print(f\"‚úì LightGBM - RMSE: {rmse_lgb:.2f}\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"üê± [3/3] CatBoost...\")\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=500, depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, random_state=42, verbose=False\n",
    ")\n",
    "cat_model.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "              early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_val_pred_cat = cat_model.predict(X_val)\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_val, y_val_pred_cat))\n",
    "results['CatBoost'] = {'RMSE': rmse_cat, 'R¬≤': r2_score(y_val, y_val_pred_cat)}\n",
    "print(f\"‚úì CatBoost - RMSE: {rmse_cat:.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ensemble (moyenne pond√©r√©e)\n",
    "print(\"üéØ [4/4] Ensemble...\")\n",
    "weights = {'XGBoost': 1/rmse_xgb, 'LightGBM': 1/rmse_lgb, 'CatBoost': 1/rmse_cat}\n",
    "total = sum(weights.values())\n",
    "weights = {k: v/total for k, v in weights.items()}\n",
    "\n",
    "y_val_pred_ensemble = (\n",
    "    weights['XGBoost'] * y_val_pred_xgb +\n",
    "    weights['LightGBM'] * y_val_pred_lgb +\n",
    "    weights['CatBoost'] * y_val_pred_cat\n",
    ")\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_val, y_val_pred_ensemble))\n",
    "results['Ensemble'] = {'RMSE': rmse_ensemble, 'R¬≤': r2_score(y_val, y_val_pred_ensemble)}\n",
    "print(f\"‚úì Ensemble - RMSE: {rmse_ensemble:.2f}\")\n",
    "\n",
    "print(f\"\\nPoids: XGB={weights['XGBoost']:.3f}, LGB={weights['LightGBM']:.3f}, CAT={weights['CatBoost']:.3f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPARAISON DES R√âSULTATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_results = pd.DataFrame(results).T.sort_values('RMSE')\n",
    "print(\"\\nüìä PERFORMANCES SUR VALIDATION (2011):\")\n",
    "print(df_results)\n",
    "\n",
    "best_model = df_results['RMSE'].idxmin()\n",
    "best_rmse = df_results.loc[best_model, 'RMSE']\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model} (RMSE={best_rmse:.2f})\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df_results['RMSE'].sort_values().plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Comparaison des Mod√®les (RMSE sur Validation 2011)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('RMSE')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "feature_importance.head(15).set_index('feature')['importance'].sort_values().plot(\n",
    "    kind='barh', ax=ax, color='coral'\n",
    ")\n",
    "ax.set_title('Top 15 Features (XGBoost)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. R√âENTRA√éNEMENT FINAL (2004-2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"üîÑ R√©entra√Ænement sur TOUTES les donn√©es (2004-2011)...\")\n",
    "\n",
    "X_full = df_train_full[available_features]\n",
    "y_full = df_train_full['TauxGrippe']\n",
    "X_full = pd.DataFrame(imputer.fit_transform(X_full), columns=X_full.columns, index=X_full.index)\n",
    "\n",
    "# XGBoost\n",
    "print(\"üöÄ XGBoost...\")\n",
    "xgb_final = xgb.XGBRegressor(\n",
    "    n_estimators=500, max_depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, min_child_weight=3,\n",
    "    gamma=0.1, random_state=42, n_jobs=-1\n",
    ")\n",
    "xgb_final.fit(X_full, y_full, verbose=False)\n",
    "\n",
    "# LightGBM\n",
    "print(\"‚ö° LightGBM...\")\n",
    "lgb_final = lgb.LGBMRegressor(\n",
    "    n_estimators=500, max_depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, min_child_samples=20,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "lgb_final.fit(X_full, y_full)\n",
    "\n",
    "# CatBoost\n",
    "print(\"üê± CatBoost...\")\n",
    "cat_final = CatBoostRegressor(\n",
    "    iterations=500, depth=7, learning_rate=0.05,\n",
    "    subsample=0.8, random_state=42, verbose=False\n",
    ")\n",
    "cat_final.fit(X_full, y_full, verbose=False)\n",
    "\n",
    "print(\"\\n‚úì Mod√®les finaux entra√Æn√©s!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SAUVEGARDE DES MOD√àLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sauvegarder les mod√®les\n",
    "with open('xgb_final.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_final, f)\n",
    "with open('lgb_final.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_final, f)\n",
    "with open('cat_final.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_final, f)\n",
    "with open('imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(imputer, f)\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights, f)\n",
    "    \n",
    "# Sauvegarder la liste des features\n",
    "with open('features.pkl', 'wb') as f:\n",
    "    pickle.dump(available_features, f)\n",
    "\n",
    "print(\"‚úì Mod√®les sauvegard√©s:\")\n",
    "print(\"  - xgb_final.pkl\")\n",
    "print(\"  - lgb_final.pkl\")\n",
    "print(\"  - cat_final.pkl\")\n",
    "print(\"  - imputer.pkl\")\n",
    "print(\"  - weights.pkl\")\n",
    "print(\"  - features.pkl\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. R√âSUM√â FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES:\")\n",
    "print(f\"   Train: {len(df_train)} obs (2004-2010)\")\n",
    "print(f\"   Validation: {len(df_val)} obs (2011)\")\n",
    "print(f\"   Features: {len(available_features)}\")\n",
    "\n",
    "print(f\"\\nüéØ PERFORMANCES (validation 2011):\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"   {model:12s}: RMSE={metrics['RMSE']:6.2f} | R¬≤={metrics['R¬≤']:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model} (RMSE={best_rmse:.2f})\")\n",
    "\n",
    "print(f\"\\nüí° PROCHAINE √âTAPE:\")\n",
    "print(f\"   Ex√©cutez KAGGLE_PREDICT.ipynb pour g√©n√©rer les pr√©dictions!\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
