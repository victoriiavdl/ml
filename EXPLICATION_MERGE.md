# EXPLICATION COMPL√àTE DU MERGE DES DONN√âES

## üìã Vue d'ensemble

### Fichiers sources
1. **data_origin/train.csv** : Donn√©es de taux de grippe par r√©gion et semaine
2. **data_origin/ListedesStationsMeteo.csv** : Liste des 62 stations m√©t√©o avec coordonn√©es
3. **DonneesMeteorologiques/synop.AAAAMM.csv** : ~96 fichiers de donn√©es m√©t√©o (2004-2011)

### Objectif
Cr√©er un fichier unique qui contient :
- Les donn√©es de grippe (train.csv)
- Les donn√©es m√©t√©orologiques correspondantes par r√©gion et semaine

---

## üîß LA D√âMARCHE COMPL√àTE

### PROBL√àME PRINCIPAL √Ä R√âSOUDRE

Les donn√©es ne sont PAS au m√™me niveau de granularit√© :

| Fichier | Granularit√© | Format |
|---------|-------------|---------|
| train.csv | **R√©gion + Semaine** | region_name, week (AAAASS) |
| synop.csv | **Station + Date/Heure** | numer_sta, date (AAAAMMJJHHMMSS) |

**Il faut donc :**
1. ‚úÖ Mapper les stations m√©t√©o aux r√©gions
2. ‚úÖ Convertir les dates en semaines
3. ‚úÖ Agr√©ger les donn√©es m√©t√©o par r√©gion et semaine

---

## üìç √âTAPE 1 : Mapping R√©gion ‚Üî Station M√©t√©o

### Le d√©fi
- train.csv a 22 r√©gions fran√ßaises (anciennes r√©gions avant 2016)
- ListedesStationsMeteo.csv a 62 stations m√©t√©o
- **Quelle(s) station(s) repr√©sentent chaque r√©gion ?**

### La solution : Mapping manuel

J'ai cr√©√© un mapping bas√© sur la g√©ographie fran√ßaise :

```python
REGION_STATION_MAPPING = {
    'ALSACE': ['07190', '07280'],  # Strasbourg
    'AQUITAINE': ['07510', '07630'],  # Bordeaux, Biarritz
    'AUVERGNE': ['07460', '07380'],  # Clermont-Ferrand
    'BASSE-NORMANDIE': ['07027', '07139'],  # Caen
    'BRETAGNE': ['07110', '07117', '07130'],  # Brest, Rennes
    'ILE-DE-FRANCE': ['07150', '07149'],  # Paris, Orly
    # ... etc pour les 22 r√©gions
}
```

**Pourquoi plusieurs stations par r√©gion ?**
- Pour avoir une meilleure couverture g√©ographique
- Pour lisser les valeurs extr√™mes locales
- Pour avoir plus de donn√©es (certaines stations ont des mesures manquantes)

---

## üìÖ √âTAPE 2 : Conversion Date ‚Üí Semaine

### Le d√©fi
```
synop : date = "20110105143000" (5 janvier 2011 √† 14h30)
train : week = 201101 (semaine 1 de 2011)
```

### La solution : ISO Calendar

```python
# Convertir en datetime
df['date'] = pd.to_datetime(df['date'], format='%Y%m%d%H%M%S')

# Extraire ann√©e et semaine ISO
df['year'] = df['date'].dt.isocalendar().year
df['week'] = df['date'].dt.isocalendar().week

# Cr√©er le code semaine AAAASS
df['week_year'] = df['year'] * 100 + df['week']
```

**Note importante** : On utilise l'ISO calendar (norme internationale) o√π :
- Semaine 1 = premi√®re semaine avec au moins 4 jours en janvier
- Les semaines vont du lundi au dimanche

---

## üìä √âTAPE 3 : Agr√©gation des donn√©es m√©t√©o

### Le d√©fi
Les donn√©es synop sont **tr√®s granulaires** :
- Plusieurs mesures par jour (toutes les 3h ou 6h)
- Par station individuelle

On veut : **UNE valeur par r√©gion et par semaine**

### La solution : Agr√©gation en 2 temps

#### 1Ô∏è‚É£ Filtrer par stations d'int√©r√™t
```python
# On ne garde que les ~40 stations qui correspondent √† nos 22 r√©gions
df_synop = df_synop[df_synop['numer_sta'].isin(stations_of_interest)]
```

**Pourquoi ?** R√©duire la m√©moire (550MB ‚Üí ~100MB)

#### 2Ô∏è‚É£ Grouper et moyenner

```python
# Merger avec le mapping station ‚Üí r√©gion
df_synop = df_synop.merge(df_station_region, on='numer_sta')

# Grouper par (r√©gion, semaine) et calculer la moyenne
df_agg = df_synop.groupby(['region_name', 'week_year']).agg({
    't': 'mean',      # Temp√©rature moyenne
    'u': 'mean',      # Humidit√© moyenne
    'rr24': 'mean',   # Pr√©cipitations moyennes
    # ... etc pour ~30 variables m√©t√©o
})
```

**R√©sultat** : Une ligne par (r√©gion, semaine) avec les moyennes m√©t√©o

---

## üîó √âTAPE 4 : Merge final

### La jointure

```python
df_final = df_train.merge(
    df_meteo_agg,
    left_on=['region_name', 'week'],
    right_on=['region_name', 'week_year'],
    how='inner'  # On garde seulement les correspondances parfaites
)
```

**Cl√©s de jointure** :
- `region_name` (normalis√© en MAJUSCULES)
- `week` (train.csv) = `week_year` (synop agr√©g√©)

**Type de jointure : INNER**
- On ne garde que les lignes o√π on a BOTH les donn√©es de grippe ET les donn√©es m√©t√©o
- R√©sultat : ~9000-9500 lignes (sur 9195 dans train.csv)

---

## üìà R√âSULTAT ATTENDU

### Structure du fichier final

| Colonne | Source | Description |
|---------|--------|-------------|
| Id | train.csv | Identifiant unique |
| week | train.csv | Semaine au format AAAASS |
| region_code | train.csv | Code num√©rique r√©gion |
| region_name | train.csv | Nom de la r√©gion |
| **TauxGrippe** | train.csv | **VARIABLE CIBLE** |
| t | synop (agr√©g√©) | Temp√©rature moyenne (¬∞K) |
| td | synop (agr√©g√©) | Point de ros√©e (¬∞K) |
| u | synop (agr√©g√©) | Humidit√© (%) |
| ff | synop (agr√©g√©) | Vitesse du vent (m/s) |
| rr24 | synop (agr√©g√©) | Pr√©cipitations 24h (mm) |
| ... | synop (agr√©g√©) | ~25 autres variables m√©t√©o |

**Total** : environ 35-40 colonnes

---

## ‚ö†Ô∏è POINTS D'ATTENTION

### 1. Valeurs manquantes
Les fichiers synop contiennent beaucoup de "mq" (mesure manquante) :
```python
# Convertir 'mq' en NaN
df[var] = pd.to_numeric(df[var], errors='coerce')
```

**Gestion** :
- Lors de l'agr√©gation, les NaN sont ignor√©s automatiquement par `.mean()`
- Apr√®s le merge, on peut imputer les NaN restants par la m√©diane

### 2. Normalisation des noms de r√©gions
```python
# train.csv peut avoir "Ile-de-France", "ILE-DE-FRANCE", etc.
df['region_name'] = df['region_name'].str.upper().str.strip()
```

### 3. Taux de couverture
Apr√®s le merge INNER, on peut perdre quelques lignes :
- Certaines semaines n'ont pas de donn√©es m√©t√©o
- Certaines r√©gions ont des gaps dans les mesures

**R√©sultat typique** : 95-100% de couverture

---

## üöÄ COMMENT UTILISER LE NOTEBOOK

### 1. Ouvrir le notebook
```bash
jupyter notebook MERGE_WEATHER_DATA.ipynb
```

### 2. Ex√©cuter toutes les cellules
- Cell ‚Üí Run All
- Dur√©e : ~2-5 minutes (selon les donn√©es)

### 3. R√©sultat
```
data_plus/train_weather_merged_complete.csv
```

Un fichier pr√™t pour :
- ‚úÖ Analyse exploratoire
- ‚úÖ Feature engineering (lags, moyennes mobiles)
- ‚úÖ Machine Learning (pr√©diction du TauxGrippe)

---

## üìö R√âF√âRENCES

### Variables m√©t√©o importantes (bas√©es sur le notebook NETTOYAGE_DONNEES)

**Top 8 variables** (corr√©l√©es avec TauxGrippe) :
1. `tminsol` - Temp√©rature min du sol
2. `t` - Temp√©rature
3. `td` - Point de ros√©e
4. `u` - Humidit√©
5. `ff` - Vitesse du vent
6. `vv` - Visibilit√© horizontale
7. `n` - N√©bulosit√© totale
8. `nbas` - N√©bulosit√© basse

### Format des dates
- **train.csv** : `week` = AAAASS (ex: 201101 = semaine 1 de 2011)
- **synop** : `date` = AAAAMMJJHHMMSS (ex: 20110105143000)

### Unit√©s m√©t√©o
- Temp√©rature (t, td, tminsol) : en Kelvin (K)
- Vent (ff) : m/s
- Humidit√© (u) : %
- Pr√©cipitations (rr1, rr6, rr24) : mm
- Pression (pres) : Pascal

---

## ‚úÖ VALIDATION DU MERGE

Apr√®s le merge, v√©rifier :

```python
# 1. Nombre de lignes
print(f"train.csv : {len(df_train)} lignes")
print(f"Apr√®s merge : {len(df_final)} lignes")
print(f"Couverture : {len(df_final)/len(df_train)*100:.1f}%")

# 2. Pas de doublons
duplicates = df_final.duplicated(subset=['region_code', 'week'])
print(f"Doublons : {duplicates.sum()}")  # Doit √™tre 0

# 3. Toutes les r√©gions pr√©sentes
print(f"R√©gions : {df_final['region_name'].nunique()}")  # Doit √™tre 22

# 4. Valeurs manquantes
missing = df_final.isnull().sum()
print(missing[missing > 0].sort_values(ascending=False))
```

---

## üéØ PROCHAINES √âTAPES

Apr√®s avoir g√©n√©r√© le fichier merg√©, vous pouvez :

1. **Nettoyer les donn√©es** (voir NETTOYAGE_DONNEES.ipynb)
   - Supprimer les colonnes avec >30% de NaN
   - Imputer les valeurs manquantes
   - D√©tecter et traiter les outliers

2. **Feature Engineering**
   - Cr√©er des lags (TauxGrippe_lag1, lag2, etc.)
   - Moyennes mobiles (ma4, ma8)
   - Variables de saison

3. **Mod√©lisation**
   - Random Forest
   - XGBoost
   - LSTM (pour les s√©ries temporelles)

---

**Auteur** : Claude
**Date** : 2024
**Projet** : Pr√©diction du taux de grippe avec donn√©es m√©t√©o
